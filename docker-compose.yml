version: '3.8'

services:
  tts-api:
    image: ghcr.io/aleckarfonta/speaker:latest
    build: .
    ports:
      - "8012:8000"
    environment:
      - ENVIRONMENT=development
      - PYTHONPATH=/app:/app/GLM-TTS
      - HF_HOME=/app/model_cache/huggingface
      - TORCH_HOME=/app/model_cache/torch
      - TRANSFORMERS_CACHE=/app/model_cache/transformers
      - NVIDIA_VISIBLE_DEVICES=all
      - COQUI_TOS_AGREED=1
      - ORT_LOGGING_LEVEL=3  # Suppress ONNX fallback warnings (campplus model)
      # TTS Backend: "xtts" or "glm-tts"
      - TTS_BACKEND=glm-tts
      - GLM_TTS_QUANTIZATION=none
      - GLM_TTS_ATTENTION=flash_attention_2
      - GLM_TTS_FLOW_STEPS=6
      - GLM_TTS_SAMPLING=18
      - GLM_TTS_TORCH_COMPILE=false  # Not needed with vLLM
      # Use vLLM engine for high-performance inference
      - GLM_TTS_ENGINE=vllm
      # GLM-TTS LLM Optimization (uncomment to customize):
      # GLM_TTS_DTYPE: fp32, fp16, bf16 (default: fp16)
      # - GLM_TTS_DTYPE=fp16
      # GLM_TTS_QUANTIZATION: none, 4bit, 8bit (default: none) - 4bit ~3x faster
      # - GLM_TTS_QUANTIZATION=4bit
      # GLM_TTS_ATTENTION: eager, sdpa, flash_attention_2 (default: eager)
      # - GLM_TTS_ATTENTION=sdpa
      #
      # Speed Tuning (uncomment to customize):
      # GLM_TTS_FLOW_STEPS: 1-20, lower=faster (default: 10, try 5-8 for speed)
      # - GLM_TTS_FLOW_STEPS=8
      # GLM_TTS_SAMPLING: top-k sampling, lower=faster (default: 25, try 15-20)
      # - GLM_TTS_SAMPLING=20
      # GLM_TTS_TORCH_COMPILE: true/false - JIT compile for ~1.5x speedup (default: false)
      # - GLM_TTS_TORCH_COMPILE=true
      # GLM_TTS_MAX_TOKEN_RATIO: max audio tokens per text char (default: 20, try 15)
      # - GLM_TTS_MAX_TOKEN_RATIO=15
    volumes:
      # Mount GLM-TTS model checkpoints (download separately: ~8GB)
      - ./GLM-TTS/ckpt:/app/GLM-TTS/ckpt:ro
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command: >
      bash -c "./scripts/start.sh"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - tts-network

  frontend:
    image: ghcr.io/aleckarfonta/speaker-frontend:latest
    build: ./frontend
    ports:
      - "3012:80"
    depends_on:
      - tts-api
    networks:
      - tts-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Prometheus for metrics collection
  prometheus:
    build:
      context: ./k8s/monitoring
      dockerfile: Dockerfile.prometheus
    ports:
      - "9199:9090"
    volumes:
      - prometheus_data:/prometheus
    networks:
      - tts-network
    depends_on:
      - tts-api
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Grafana for metrics visualization
  grafana:
    build:
      context: ./k8s/monitoring
      dockerfile: Dockerfile.grafana
    ports:
      - "3333:3000"
    volumes:
      - grafana_data:/var/lib/grafana
    networks:
      - tts-network
    depends_on:
      - prometheus
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

volumes:
  prometheus_data:
  grafana_data:

networks:
  tts-network:
    driver: bridge
